{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6a7b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  animal_code species      diet  m_antiox\n",
      "0        pig1   line1    antiox        50\n",
      "1        pig2   line2  standard        45\n",
      "2        pig3   line2    antiox        54\n",
      "3        pig4   line1    antiox        34\n",
      "root\n",
      " |-- animal_code: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- m_antiox: long (nullable = true)\n",
      "\n",
      "+-----------+-------+--------+--------+\n",
      "|animal_code|species|    diet|m_antiox|\n",
      "+-----------+-------+--------+--------+\n",
      "|       pig1|  line1|  antiox|      50|\n",
      "|       pig2|  line2|standard|      45|\n",
      "|       pig3|  line2|  antiox|      54|\n",
      "|       pig4|  line1|  antiox|      34|\n",
      "+-----------+-------+--------+--------+\n",
      "\n",
      "root\n",
      " |-- animal_code: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- m_antiox: integer (nullable = true)\n",
      "\n",
      "+-----------+-------+--------+--------+\n",
      "|animal_code|species|    diet|m_antiox|\n",
      "+-----------+-------+--------+--------+\n",
      "|       pig1|  line1|  antiox|      50|\n",
      "|       pig2|  line2|standard|      45|\n",
      "|       pig3|  line2|  antiox|      54|\n",
      "|       pig4|  line1|  antiox|      34|\n",
      "+-----------+-------+--------+--------+\n",
      "\n",
      "<bound method PandasConversionMixin.toPandas of DataFrame[animal_code: string, species: string, diet: string, m_antiox: int]>\n",
      "true\n",
      "true\n"
     ]
    }
   ],
   "source": [
    " \n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd   \n",
    "\n",
    "data = [['pig1', 'line1','antiox', 50], ['pig2','line2','standard', 45],\n",
    "        ['pig3','line2','antiox', 54],['pig4','line1','antiox',34]] \n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "pandasDF = pd.DataFrame(data, columns = ['animal_code', 'species','diet','m_antiox']) \n",
    "  \n",
    "# print dataframe. \n",
    "print(pandasDF)\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"pandas-dataset\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sparkDF=spark.createDataFrame(pandasDF) \n",
    "sparkDF.printSchema()\n",
    "sparkDF.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \n",
    "mySchema = StructType([ StructField(\"animal_code\", StringType(), True)\\\n",
    "                         , StructField(\"species\", StringType(), True)\\\n",
    "                         ,StructField(\"diet\", StringType(), True)\\\n",
    "                         ,StructField(\"m_antiox\", IntegerType(), True)])\n",
    "\n",
    "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
    "\n",
    "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
    "print(pandasDF2)\n",
    "\n",
    "\n",
    "test=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\n",
    "print(test)\n",
    "\n",
    "test123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\n",
    "print(test123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9042333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+--------+\n",
      "|animal_code|species|    diet|m_antiox|\n",
      "+-----------+-------+--------+--------+\n",
      "|          1|  line1|  antiox|      55|\n",
      "|          2|  line2|standard|      45|\n",
      "|          3|  line2|  antiox|      58|\n",
      "|          4|  line1|  antiox|      33|\n",
      "|          5|  line2|standard|      45|\n",
      "|          6|  line1|standard|      51|\n",
      "|          7|  line1|  antiox|      47|\n",
      "|          8|  line2|  antiox|      52|\n",
      "|          9|  line2|standard|      34|\n",
      "|         10|  line1|standard|      39|\n",
      "|         11|  line2|  antiox|      54|\n",
      "|         12|  line1|standard|      45|\n",
      "|         13|  line1|standard|      39|\n",
      "|         14|  line2|  antiox|      34|\n",
      "|         15|  line1|  antiox|      45|\n",
      "|         16|  line1|  antiox|      53|\n",
      "|         17|  line2|standard|      44|\n",
      "|         18|  line1|standard|      39|\n",
      "|         19|  line1|  antiox|      20|\n",
      "|         20|  line2|  antiox|      39|\n",
      "+-----------+-------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputfile=\"antiox.csv\"\n",
    "sparkDF3 = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(mySchema) \\\n",
    "      .load(inputfile)\n",
    "sparkDF3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b486477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------------+\n",
      "|species|sum_m_antiox|      avg_m_antiox|\n",
      "+-------+------------+------------------+\n",
      "|  line1|         880|41.904761904761905|\n",
      "|  line2|         859| 45.21052631578947|\n",
      "+-------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sparkDF3.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select species, sum(m_antiox) as sum_m_antiox , avg(m_antiox) as avg_m_antiox  from EMP \" +\n",
    "          \"group by species having avg_m_antiox > 40 \" + \n",
    "          \"order by sum_m_antiox desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b262f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
