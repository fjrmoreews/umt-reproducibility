{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f5e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "#sc =SparkContext()\n",
    "\n",
    "spark = SparkSession.builder.appName('ML').getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8918c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    " \n",
    " \n",
    "df = spark.read.csv(\"data-behaviour.csv\", header=True, inferSchema= True)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973e65ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+-------------+--------------+----------------+------------+-------------+---------------+--------+------+-------------+-------------+------------+-------+-------+\n",
      "|x  |age|feeding_env  |genetic_score|diet          |owner_evaluation|diet_formula|behaviour    |group_behaviour|race    |sex   |health_eval_1|health_eval_2|feeding_time|farm   |weight |\n",
      "+---+---+-------------+-------------+--------------+----------------+------------+-------------+---------------+--------+------+-------------+-------------+------------+-------+-------+\n",
      "|1  |25 |bio-protocol3|226802       |prot++fib+min+|7               |fomula2     |stressed--   |social+        |Gascon2 |Male  |0            |0            |40          |farm408|<=100kg|\n",
      "|2  |38 |bio-protocol3|89814        |prot++fib++   |9               |fomula7     |active+      |social---      |Chinese1|Male  |0            |0            |50          |farm408|<=100kg|\n",
      "|3  |28 |mixed        |336951       |carb-         |12              |fomula7     |hyper-active+|social---      |Chinese1|Male  |0            |0            |40          |farm408|>100kg |\n",
      "|4  |44 |bio-protocol3|160323       |met+cyst+     |10              |fomula7     |stressed--   |social---      |Gascon2 |Male  |7688         |0            |40          |farm408|>100kg |\n",
      "|5  |18 |?            |103497       |met+cyst+     |10              |fomula2     |?            |social+        |Chinese1|Female|0            |0            |30          |farm408|<=100kg|\n",
      "+---+---+-------------+-------------+--------------+----------------+------------+-------------+---------------+--------+------+-------------+-------------+------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- feeding_env: string (nullable = true)\n",
      " |-- genetic_score: integer (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- owner_evaluation: integer (nullable = true)\n",
      " |-- diet_formula: string (nullable = true)\n",
      " |-- behaviour: string (nullable = true)\n",
      " |-- group_behaviour: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- health_eval_1: integer (nullable = true)\n",
      " |-- health_eval_2: integer (nullable = true)\n",
      " |-- feeding_time: integer (nullable = true)\n",
      " |-- farm: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate = False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b5cff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      " |-- feeding_env: string (nullable = true)\n",
      " |-- genetic_score: integer (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- owner_evaluation: float (nullable = true)\n",
      " |-- diet_formula: string (nullable = true)\n",
      " |-- behaviour: string (nullable = true)\n",
      " |-- group_behaviour: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- health_eval_1: float (nullable = true)\n",
      " |-- health_eval_2: float (nullable = true)\n",
      " |-- feeding_time: float (nullable = true)\n",
      " |-- farm: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "\n",
    "# List of continuous features\n",
    "CONTI_FEATURES  = ['age','health_eval_1', 'owner_evaluation', 'health_eval_2', 'feeding_time']\n",
    "# Convert the type\n",
    "df = convertColumn(df, CONTI_FEATURES, FloatType())\n",
    "# Check the dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75f7f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "| age|genetic_score|\n",
      "+----+-------------+\n",
      "|25.0|       226802|\n",
      "|38.0|        89814|\n",
      "|28.0|       336951|\n",
      "|44.0|       160323|\n",
      "|18.0|       103497|\n",
      "+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select columns\n",
    "df.select('age','genetic_score').show(5)\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b304dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|          diet|count|\n",
      "+--------------+-----+\n",
      "|        prot--|   83|\n",
      "|    prot++min+|  247|\n",
      "|        carb++|  509|\n",
      "|          met+|  594|\n",
      "|   prot++min--|  657|\n",
      "|         vitA+|  834|\n",
      "|   prot++lip--|  955|\n",
      "|         prot+| 1389|\n",
      "|         carb-| 1601|\n",
      "|prot++fib+min+| 1812|\n",
      "|  prot++carb--| 2061|\n",
      "|         lip++| 2657|\n",
      "|        carb--| 8025|\n",
      "|     met+cyst+|10878|\n",
      "|   prot++fib++|16540|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count by group\n",
    "df.groupBy(\"diet\").count().sort(\"count\",ascending=True).show()\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6212e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------+------------------+------------+----------+---------------+--------+------+------------------+------------------+------------------+-----+-------+\n",
      "|summary|                 x|               age|       feeding_env|     genetic_score|  diet|  owner_evaluation|diet_formula| behaviour|group_behaviour|    race|   sex|     health_eval_1|     health_eval_2|      feeding_time| farm| weight|\n",
      "+-------+------------------+------------------+------------------+------------------+------+------------------+------------+----------+---------------+--------+------+------------------+------------------+------------------+-----+-------+\n",
      "|  count|             48842|             48842|             48842|             48842| 48842|             48842|       48842|     48842|          48842|   48842| 48842|             48842|             48842|             48842|48842|  48842|\n",
      "|   mean|           24421.5| 38.64358543876172|              null|189664.13459727284|  null|10.078088530363212|        null|      null|           null|    null|  null|1079.0676262233324| 87.50231358257237|40.422382375824085| null|   null|\n",
      "| stddev|14099.615260708357|13.710509934443511|              null|105604.02542315763|  null|2.5709727555922557|        null|      null|           null|    null|  null| 7452.019057655415|403.00455212435924|12.391444024252285| null|   null|\n",
      "|    min|                 1|              17.0|                 ?|             12285|carb++|               1.0|     fomula1|         ?|        social+|Chinese1|Female|               0.0|               0.0|               1.0|    ?|<=100kg|\n",
      "|    max|             48842|              90.0|traditionnal-low-1|           1490400| vitA+|              16.0|     fomula7|stressed--|      social---| Gascon2|  Male|           99999.0|            4356.0|              99.0|farm9| >100kg|\n",
      "+-------+------------------+------------------+------------------+------------------+------+------------------+------------+----------+---------------+--------+------+------------------+------------------+------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Describe the data\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cfeb713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      health_eval_1|\n",
      "+-------+------------------+\n",
      "|  count|             48842|\n",
      "|   mean|1079.0676262233324|\n",
      "| stddev| 7452.019057655418|\n",
      "|    min|               0.0|\n",
      "|    max|           99999.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('health_eval_1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e895045f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+\n",
      "|age_sex|Female|Male|\n",
      "+-------+------+----+\n",
      "|   17.0|   295| 300|\n",
      "|   18.0|   427| 435|\n",
      "|   19.0|   514| 539|\n",
      "|   20.0|   535| 578|\n",
      "|   21.0|   502| 594|\n",
      "|   22.0|   521| 657|\n",
      "|   23.0|   547| 782|\n",
      "|   24.0|   478| 728|\n",
      "|   25.0|   466| 729|\n",
      "|   26.0|   422| 731|\n",
      "|   27.0|   429| 803|\n",
      "|   28.0|   446| 834|\n",
      "|   29.0|   420| 803|\n",
      "|   30.0|   406| 872|\n",
      "|   31.0|   426| 899|\n",
      "|   32.0|   373| 880|\n",
      "|   33.0|   379| 956|\n",
      "|   34.0|   402| 901|\n",
      "|   35.0|   366| 971|\n",
      "|   36.0|   423| 925|\n",
      "+-------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Crosstab computation\n",
    "#In some occasion, it can be interesting to see the descriptive statistics between two pairwise columns. For instance, you can count the number of people with weight below or above 50k by diet level.\n",
    "#This operation is called a crosstab. \n",
    "df.crosstab('age', 'sex').sort(\"age_sex\").show()\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7498a81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x',\n",
       " 'age',\n",
       " 'feeding_env',\n",
       " 'genetic_score',\n",
       " 'diet',\n",
       " 'diet_formula',\n",
       " 'behaviour',\n",
       " 'group_behaviour',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'health_eval_1',\n",
       " 'health_eval_2',\n",
       " 'feeding_time',\n",
       " 'farm',\n",
       " 'weight']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('owner_evaluation').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c1724ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20211"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter data\n",
    "df.filter(df.age > 40).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "235fa404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      " |-- feeding_env: string (nullable = true)\n",
      " |-- genetic_score: integer (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- owner_evaluation: float (nullable = true)\n",
      " |-- diet_formula: string (nullable = true)\n",
      " |-- behaviour: string (nullable = true)\n",
      " |-- group_behaviour: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- health_eval_1: float (nullable = true)\n",
      " |-- health_eval_2: float (nullable = true)\n",
      " |-- feeding_time: float (nullable = true)\n",
      " |-- farm: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      " |-- age-square: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 2) Data preprocessing\n",
    "#Add age square\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 1 Select the column\n",
    "age_square = df.select(col(\"age\")**2)\n",
    "\n",
    "# 2 Apply the transformation and add it to the DataFrame\n",
    "df = df.withColumn(\"age-square\", col(\"age\")**2)\n",
    "\n",
    "df.printSchema()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da24218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=25.0, age-square=625.0, feeding_env='bio-protocol3', genetic_score=226802, diet='prot++fib+min+', owner_evaluation=7.0, diet_formula='fomula2', behaviour='stressed--', group_behaviour='social+', race='Gascon2', sex='Male', health_eval_1=0.0, health_eval_2=0.0, feeding_time=40.0, farm='farm408', weight='<=100kg')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS = ['age', 'age-square', 'feeding_env', 'genetic_score', 'diet', 'owner_evaluation', 'diet_formula',\n",
    "           'behaviour', 'group_behaviour', 'race', 'sex', 'health_eval_1', 'health_eval_2',\n",
    "           'feeding_time', 'farm','weight']\n",
    "df = df.select(COLUMNS)\n",
    "df.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ec86354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                farm|count(farm)|\n",
      "+--------------------+-----------+\n",
      "|            farm3267|         21|\n",
      "|Outlying-US(Guam-...|         23|\n",
      "|             farm328|         30|\n",
      "|             farm327|         37|\n",
      "|              farm40|         51|\n",
      "|               farm5|         65|\n",
      "|             farm323|         67|\n",
      "|              farm33|         68|\n",
      "|             farm321|         75|\n",
      "|            farm3264|         87|\n",
      "|             farm402|        103|\n",
      "|             farm322|        106|\n",
      "|              farm39|        108|\n",
      "|             farm409|        116|\n",
      "|               farm9|        122|\n",
      "|             farm326|        126|\n",
      "|              farm19|        155|\n",
      "|             farm400|        184|\n",
      "|             farm325|        242|\n",
      "|              farm36|        250|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "0\n",
      "48842\n",
      "+--------------------+-----------+\n",
      "|                farm|count(farm)|\n",
      "+--------------------+-----------+\n",
      "|            farm3267|         21|\n",
      "|Outlying-US(Guam-...|         23|\n",
      "|             farm328|         30|\n",
      "|             farm327|         37|\n",
      "|              farm40|         51|\n",
      "|               farm5|         65|\n",
      "|             farm323|         67|\n",
      "|              farm33|         68|\n",
      "|             farm321|         75|\n",
      "|            farm3264|         87|\n",
      "|             farm402|        103|\n",
      "|             farm322|        106|\n",
      "|              farm39|        108|\n",
      "|             farm409|        116|\n",
      "|               farm9|        122|\n",
      "|             farm326|        126|\n",
      "|              farm19|        155|\n",
      "|             farm400|        184|\n",
      "|             farm325|        242|\n",
      "|              farm36|        250|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('farm').agg({'farm': 'count'}).sort(asc(\"count(farm)\")).show()\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(df.filter(col('farm') == 'farm12').count())\n",
    "print(df.filter(col('farm') != 'farm12').count())\n",
    "\n",
    "df_remove = df.filter(col('farm') !=\t'farm12')\n",
    "df_remove.groupby('farm').agg({'farm': 'count'}).sort(asc(\"count(farm)\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bc27969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3) Build a data processing pipeline\n",
    "#First of all, you select the string column to index.\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "stringIndexer = StringIndexer(inputCol=\"feeding_env\", outputCol=\"feeding_env_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd721b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------------+-------------+--------------+----------------+------------+----------+---------------+--------+----+-------------+-------------+------------+-------+-------+-------------------+-------------+\n",
      "| age|age-square|  feeding_env|genetic_score|          diet|owner_evaluation|diet_formula| behaviour|group_behaviour|    race| sex|health_eval_1|health_eval_2|feeding_time|   farm| weight|feeding_env_encoded|workclass_vec|\n",
      "+----+----------+-------------+-------------+--------------+----------------+------------+----------+---------------+--------+----+-------------+-------------+------------+-------+-------+-------------------+-------------+\n",
      "|25.0|     625.0|bio-protocol3|       226802|prot++fib+min+|             7.0|     fomula2|stressed--|        social+| Gascon2|Male|          0.0|          0.0|        40.0|farm408|<=100kg|                0.0|(9,[0],[1.0])|\n",
      "|38.0|    1444.0|bio-protocol3|        89814|   prot++fib++|             9.0|     fomula7|   active+|      social---|Chinese1|Male|          0.0|          0.0|        50.0|farm408|<=100kg|                0.0|(9,[0],[1.0])|\n",
      "+----+----------+-------------+-------------+--------------+----------------+------------+----------+---------------+--------+----+-------------+-------------+------------+-------+-------+-------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fit the data and transform it\n",
    "model = stringIndexer.fit(df) \n",
    "indexed = model.transform(df)\n",
    " \n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"feeding_env_encoded\", outputCol=\"workclass_vec\")\n",
    "encoder.setDropLast(False)\n",
    "ohe = encoder.fit(indexed)\n",
    "encoded = ohe.transform(indexed)\n",
    "encoded.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f01f7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the pipeline\n",
    "#Encode the categorical data\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "CATE_FEATURES = ['feeding_env', 'diet', 'diet_formula', 'behaviour', 'group_behaviour', 'race', 'sex', 'farm','weight']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16ebe4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Index the label feature \n",
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx =  StringIndexer(inputCol=\"weight\", outputCol=\"newlabel\")\n",
    "stages += [label_stringIdx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c0921f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Add continuous variable \n",
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ba404ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Assemble the steps.\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97c364bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df_remove)\n",
    "model = pipelineModel.transform(df_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a8fa8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=25.0, age-square=625.0, feeding_env='bio-protocol3', genetic_score=226802, diet='prot++fib+min+', owner_evaluation=7.0, diet_formula='fomula2', behaviour='stressed--', group_behaviour='social+', race='Gascon2', sex='Male', health_eval_1=0.0, health_eval_2=0.0, feeding_time=40.0, farm='farm408', weight='<=100kg', feeding_envIndex=0.0, feeding_envclassVec=SparseVector(8, {0: 1.0}), dietIndex=5.0, dietclassVec=SparseVector(14, {5: 1.0}), diet_formulaIndex=1.0, diet_formulaclassVec=SparseVector(6, {1: 1.0}), behaviourIndex=6.0, behaviourclassVec=SparseVector(14, {6: 1.0}), group_behaviourIndex=2.0, group_behaviourclassVec=SparseVector(5, {2: 1.0}), raceIndex=1.0, raceclassVec=SparseVector(4, {1: 1.0}), sexIndex=0.0, sexclassVec=SparseVector(1, {0: 1.0}), farmIndex=0.0, farmclassVec=SparseVector(25, {0: 1.0}), weightIndex=0.0, weightclassVec=SparseVector(1, {0: 1.0}), newlabel=0.0, features=SparseVector(83, {0: 1.0, 13: 1.0, 23: 1.0, 34: 1.0, 44: 1.0, 48: 1.0, 51: 1.0, 52: 1.0, 77: 1.0, 78: 25.0, 80: 7.0, 82: 40.0}))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84715877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  1.0|[0.0,0.0,1.0,0.0,...|\n",
      "|  1.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[0.0,0.0,0.0,1.0,...|\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[0.0,0.0,0.0,1.0,...|\n",
      "|  1.0|[0.0,1.0,0.0,0.0,...|\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  1.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[0.0,0.0,0.0,1.0,...|\n",
      "|  1.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  1.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|  1.0|[1.0,0.0,0.0,0.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 4) Build the classifier: logistic\n",
    "\n",
    "# the computation faster, you convert model to a DataFrame.\n",
    "\n",
    "#You need to select newlabel and features from model using map.\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"newlabel\"], DenseVector(x[\"features\"])))\n",
    "\n",
    "#You are ready to create the train data as a DataFrame. You use the sqlContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df_train = sqlContext.createDataFrame(input_data, [\"label\", \"features\"])\t\t\t\n",
    "\n",
    "#Check the second row\n",
    "\n",
    "df_train.show(20)\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e32a576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|       29695|\n",
      "|  1.0|        9330|\n",
      "+-----+------------+\n",
      "\n",
      "-------\n",
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|        7460|\n",
      "|  1.0|        2357|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a train/test set\n",
    "\n",
    "#You split the dataset 80/20 with randomSplit.\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)\n",
    "\n",
    "#Let's count how many animals with weight below/above 50k in both training and test set\n",
    "print(\"-------\")\n",
    "train_data.groupby('label').agg({'label': 'count'}).show()\n",
    "\n",
    "print(\"-------\")\n",
    "\n",
    "test_data.groupby('label').agg({'label': 'count'}).show()\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca827ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.031721754703633705,-0.046050456282018926,0.016849281639062078,-0.12673294214055283,-0.027771853561445556,0.20791567156831212,0.15335028817708612,-0.23327172280506156,-0.11810799681898179,-0.03995810640274398,0.16362133114249713,0.2711573185369212,0.0034731472384288927,-0.1744675521134223,0.016435815893512465,-0.18270790429639847,-0.2040479195348422,0.36998449004987444,-0.1460989894403246,0.3640855774700959,-0.16705498370284466,-0.1911950566602864,0.29254408683280525,-0.24492727724865668,-0.12678671546460077,-0.1516469134206673,-0.11795523739266717,-0.1149863547396856,0.1810848196865257,-0.022237968325231953,0.2307301458294434,-0.07721130303743942,0.03698545739001802,-0.19997837897089632,-0.1237675033916426,-0.12715680051501446,-0.05123639541641801,-0.1765412210319142,-0.19964982666293882,0.09416038266192625,0.08255669206077007,-0.19442141068754945,0.24806764880550516,-0.13784042657078444,-0.22414949188760735,-0.1725646037904694,0.2996613832516706,0.03496100776428215,-0.09153437364625006,-0.0021451149208653385,-0.12341910675550692,0.12652843301484726,0.006225566771075158,-0.20123120933270489,-0.02441799750510084,0.033440740578383296,-0.017409631923829234,-0.044976507030564286,0.06321359744179819,-0.02099133450161039,-0.10943796489410042,-0.14792058419988252,-0.03673429602904855,-0.02607492143472017,-0.16798210126859478,-0.05475813182879657,0.09320035043962803,-0.22481898501786368,-0.05612542891162459,-0.06771236336650821,0.03763459647060521,0.026065557216951953,0.08243932268219083,0.07758329390599258,0.11632599709370413,-0.24092156149504665,-0.19047887788449477,-1.6707779698878975,0.005214583837831164,1.3114507434674934e-05,0.03827886299166974,0.00015058798613628915,0.005846856615922472]\n",
      "Intercept: -1.185476653757918\n"
     ]
    }
   ],
   "source": [
    "#Build the logistic regressor\n",
    "\n",
    " \n",
    "# Import `LinearRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)\n",
    "\n",
    "#You can see the coefficients from the regression\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2365f3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 5) Train and evaluate the model\n",
    "\n",
    " \n",
    "\n",
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)\n",
    "\n",
    " \n",
    "\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0148569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.95137424981967...|\n",
      "|  0.0|       0.0|[0.95951459760685...|\n",
      "|  0.0|       0.0|[0.86601705569820...|\n",
      "|  0.0|       0.0|[0.56853336294609...|\n",
      "|  0.0|       0.0|[0.60611001210972...|\n",
      "|  0.0|       0.0|[0.93937565220683...|\n",
      "|  0.0|       0.0|[0.94765619611021...|\n",
      "|  0.0|       0.0|[0.60026513504351...|\n",
      "|  0.0|       0.0|[0.90130382345788...|\n",
      "|  0.0|       0.0|[0.90829619609276...|\n",
      "|  0.0|       0.0|[0.83975005171014...|\n",
      "|  0.0|       0.0|[0.90288325974483...|\n",
      "|  0.0|       0.0|[0.85963065749982...|\n",
      "|  0.0|       0.0|[0.89592839513254...|\n",
      "|  0.0|       0.0|[0.88843828485160...|\n",
      "|  0.0|       0.0|[0.88229165578475...|\n",
      "|  0.0|       0.0|[0.91691713528470...|\n",
      "|  0.0|       0.0|[0.90407934072028...|\n",
      "|  0.0|       0.0|[0.88839898695408...|\n",
      "|  0.0|       0.0|[0.72651835008274...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "selected.show(20)\t\t\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3a337c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cm.groupby('label').agg({'label': 'count'}).show()\n",
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|        7460|\n",
      "|  1.0|        2357|\n",
      "+-----+------------+\n",
      "\n",
      "cm.groupby('prediction').agg({'prediction': 'count'}).show()\n",
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|             7740|\n",
      "|       1.0|             2077|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "'''\n",
    "You need to look at the accuracy metric to see how well (or bad) the model performs.\n",
    "Currently, there is no API to compute the accuracy measure in Spark. \n",
    "The default value is the ROC, receiver operating characteristic curve. \n",
    "It is a different metrics that take into account the false positive rate.\n",
    "\n",
    "Before you look at the ROC, let's construct the accuracy measure. You are more familiar with this metric. \n",
    "The accuracy measure is the sum of the correct prediction over the total number of observations. \n",
    "'''\n",
    "\n",
    "#You create a DataFrame with the label and the `prediction.\n",
    "\n",
    "cm = predictions.select(\"label\", \"prediction\")\t\t\t\n",
    "\n",
    "#You can check the number of class in the label and the prediction\n",
    "print(\"cm.groupby('label').agg({'label': 'count'}).show()\")\n",
    "cm.groupby('label').agg({'label': 'count'}).show()\t\t\t\n",
    "\n",
    "print(\"cm.groupby('prediction').agg({'prediction': 'count'}).show()\")\n",
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7980875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9714780482835897"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For instance, in the test set, there is 2393 animals with an weight above 100kg and 7423 below. The classifier,\n",
    "however, predicted 2109 animals with weight above 50k.\n",
    "\n",
    "You can compute the accuracy by computing the count when the label are correctly classified over the total number of rows.\n",
    "\"\"\"\n",
    "cm.filter(cm.label == cm.prediction).count() / cm.count()\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa5a1eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 97.086%\n"
     ]
    }
   ],
   "source": [
    "#You can wrap everything together and write a function to compute the accuracy. \n",
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test_data)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = linearModel)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfb52926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995942438301971\n",
      "areaUnderROC\n",
      "0.9995942438301971\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "ROC metrics\n",
    "\n",
    "The module BinaryClassificationEvaluator includes the ROC measures. \n",
    "The Receiver Operating Characteristic curve is another common tool used \n",
    "with binary classification. It is very similar to the precision/recall curve,\n",
    "but instead of plotting precision versus recall, the ROC curve shows\n",
    "the true positive rate (i.e. recall) against the false positive rate. \n",
    "The false positive rate is the ratio of negative instances that are incorrectly\n",
    "classified as positive. It is equal to one minus the true negative rate.\n",
    "The true negative rate is also called specificity.\n",
    "Hence the ROC curve plots sensitivity (recall) versus 1 - specificity \n",
    "\"\"\"\n",
    "\n",
    "### Use ROC \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(evaluator.getMetricName())\n",
    "print(evaluator.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6) Tune the hyperparameter\n",
    "\"\"\"\n",
    "Last but not least, you can tune the hyperparameters. Similar to scikit learn you create a parameter grid, and you add the parameters you want to tune.\n",
    "\n",
    "To reduce the time of the computation, you only tune the regularization parameter with only two values.\n",
    "\"\"\"\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5])\n",
    "             .build())\n",
    "\n",
    "#Finally, you evaluate the model with using the cross valiation method with 5 folds. It takes alot of time to train.\n",
    "\n",
    "from time import *\n",
    "start_time = time()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f4dd7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 1139.828 seconds\n",
      "Model accuracy: \n",
      "Model accuracy: 99.990%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_9416b3699939', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LogisticRegression_9416b3699939', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_9416b3699939', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent='LogisticRegression_9416b3699939', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='LogisticRegression_9416b3699939', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LogisticRegression_9416b3699939', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='LogisticRegression_9416b3699939', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LogisticRegression_9416b3699939', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n",
       " Param(parent='LogisticRegression_9416b3699939', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LogisticRegression_9416b3699939', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='LogisticRegression_9416b3699939', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_9416b3699939', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       " Param(parent='LogisticRegression_9416b3699939', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LogisticRegression_9416b3699939', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n",
       " Param(parent='LogisticRegression_9416b3699939', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# likely take a fair amount of time\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)\n",
    "\n",
    "#Time to train model: 978.807 seconds\n",
    "\n",
    "#The best regularization hyperparameter is 0.01, with an accuracy of 85.316 percent.\n",
    "\n",
    "print(\"Model accuracy: \")\n",
    "\n",
    "accuracy_m(model = cvModel)\n",
    "\n",
    "#You can extract the recommended parameter by chaining cvModel.bestModel with extractParamMap()\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel.extractParamMap()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17821de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
